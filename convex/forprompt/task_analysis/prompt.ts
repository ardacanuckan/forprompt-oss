/**
 * Analysis Task
 * Task instructions for comprehensive prompt quality evaluation
 *
 * Version: 1
 * Last updated: 2026-01-24T21:47:17.335Z
 *
 * @generated by ForPrompt CLI - Do not edit manually
 */

export const task_analysis = "<task>\nAnalyze the provided system prompt and deliver a comprehensive quality evaluation.\n</task>\n\n<analysis_framework>\nYour analysis must systematically cover these dimensions:\n\n## Core Metrics\n- **Clarity Score (1-10)**: How clear and unambiguous is the prompt?\n- **Overall Assessment**: Comprehensive summary of strengths, weaknesses, and quality\n\n## Improvement Areas\n- **Improvements**: Specific, actionable suggestions to enhance effectiveness\n- **Edge Cases**: Potential failure modes, unclear scenarios, or missing coverage\n- **Optimizations**: Token efficiency and performance improvements\n\n## Conditional Analysis\nIf prompt metadata is provided (purpose, behavior, constraints):\n- **Purpose Alignment**: Does the prompt achieve its stated goals?\n- **Behavior Alignment**: Does it implement expected behavior patterns?\n- **Constraints Alignment**: Are limitations and rules properly enforced?\n\nIf tools are linked:\n- **Tool Instructions**: Are tool usage patterns clearly explained?\n- **Tool Strategy**: Is the overall approach to tool usage effective?\n</analysis_framework>\n\n<evaluation_criteria>\nWhen scoring and providing feedback:\n- Be **specific** - cite exact phrases or sections that are problematic\n- Be **actionable** - every issue should have a clear fix\n- Be **prioritized** - focus on high-impact improvements first\n- Be **balanced** - acknowledge strengths, not just weaknesses\n- Consider **real-world usage** - how will this prompt perform in practice?\n</evaluation_criteria>\n\n<output_format>\nCRITICAL: Respond with ONLY valid JSON. No markdown, no explanation, no code blocks.\n\n{\n  \"clarityScore\": 8,\n  \"overallAssessment\": \"Detailed 2-4 sentence summary of the prompt's quality, strengths, and areas for improvement.\",\n  \"improvements\": [\n    \"Specific, actionable improvement with context\",\n    \"Another improvement suggestion\"\n  ],\n  \"edgeCases\": [\n    \"Potential failure mode or edge case\",\n    \"Another scenario to handle\"\n  ],\n  \"optimizations\": [\n    \"Token or performance optimization\",\n    \"Another optimization\"\n  ],\n  \"alignmentCheck\": {\n    \"purposeAlignment\": {\n      \"score\": 8,\n      \"feedback\": \"How well the prompt achieves its stated purpose\"\n    },\n    \"behaviorAlignment\": {\n      \"score\": 7,\n      \"feedback\": \"How well it implements expected behavior\"\n    },\n    \"constraintsAlignment\": {\n      \"score\": 9,\n      \"feedback\": \"How well constraints are enforced\"\n    }\n  },\n  \"toolUsageAnalysis\": {\n    \"overallToolUsage\": \"Overall assessment of tool usage strategy\",\n    \"tools\": [\n      {\n        \"name\": \"Tool Name\",\n        \"isProperlyInstructed\": true,\n        \"issues\": [\"Specific issue if any\"],\n        \"suggestions\": [\"Actionable suggestion\"]\n      }\n    ]\n  }\n}\n</output_format>\n\n<requirements>\n- `clarityScore`: Integer from 1-10\n- `overallAssessment`: Detailed paragraph (2-4 sentences)\n- All arrays: 1-3 specific, actionable items\n- `alignmentCheck`: Only include if prompt metadata is provided\n- `toolUsageAnalysis`: Only include if tools are linked\n</requirements>";

export const task_analysisInfo = {
  key: "task_analysis",
  name: "Analysis Task",
  version: 1,
  updatedAt: 1769291237335,
} as const;

export default task_analysis;
